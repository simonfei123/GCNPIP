{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "COGS185Final2.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "sqFLw7HfHBqE",
        "yDeq-2l2HF_6",
        "FwhQf83CId84"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sqFLw7HfHBqE"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3HwJDGdGgNo"
      },
      "source": [
        "!pip install torch-scatter -f https://pytorch-geometric.com/whl/torch-1.7.0+cu101.html\n",
        "!pip install torch-sparse -f https://pytorch-geometric.com/whl/torch-1.7.0+cu101.html\n",
        "!pip install torch-geometric\n",
        "!pip install -q captum\n",
        "!pip install sklearn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R9LCLdGgGqdg"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torch_geometric.nn as pyg_nn\n",
        "import torch_geometric.utils as pyg_utils\n",
        "from torch_geometric.utils import to_networkx\n",
        "\n",
        "import time\n",
        "from datetime import datetime\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import networkx as nx\n",
        "\n",
        "from torch_geometric.data import DataLoader\n",
        "from torch_geometric.data import Data\n",
        "\n",
        "import torch_geometric.transforms as T\n",
        "\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import auc\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import pickle"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mypZ5jNwG2Qm",
        "outputId": "9081abfa-1179-4958-b242-f3ac91b14bc8"
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDeq-2l2HF_6"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iwlkm1OFG-EW",
        "outputId": "8d1fb74a-0f8f-499a-e7ab-097ecf503067"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0FBZ8SCOHT_1"
      },
      "source": [
        "# use encoding=\"bytes\" to convert python2 binaries to python3\n",
        "pickle_off = open(\"/content/drive/MyDrive/COGS185Final/train.cpkl\",\"rb\")\n",
        "train_list, train_data = pickle.load(pickle_off, encoding=\"bytes\")\n",
        "pickle_off = open(\"/content/drive/MyDrive/COGS185Final/test.cpkl\",\"rb\")\n",
        "test_list, test_data = pickle.load(pickle_off, encoding=\"bytes\")"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KMrmErWcHfBF"
      },
      "source": [
        "def transformDataset(dataset_in):\n",
        "    datalist1 = []\n",
        "    datalist2 = []\n",
        "    for data in dataset_in:\n",
        "        x1 = torch.tensor(data[b'l_vertex'], dtype=torch.float) # x1\n",
        "        x2 = torch.tensor(data[b'r_vertex'], dtype=torch.float) # x2\n",
        "        nd_dim1 = data[b'l_hood_indices'].shape[0] # node dimension\n",
        "        nh_dim1 = data[b'l_hood_indices'].shape[1] # neighborhood dimension\n",
        "        nd_dim2 = data[b'r_hood_indices'].shape[0] # node dimension\n",
        "        nh_dim2 = data[b'r_hood_indices'].shape[1] # neighborhood dimension\n",
        "        sid1 = np.arange(nd_dim1).reshape(nd_dim1,1) # sid is self index\n",
        "        sid2 = np.arange(nd_dim2).reshape(nd_dim2,1) # sid is self index\n",
        "        bsid1 = sid1.copy()\n",
        "        bsid2 = sid2.copy()\n",
        "        for i in range(nh_dim1-1):\n",
        "            sid1 = np.hstack((sid1,bsid1))\n",
        "        for i in range(nh_dim2-1):\n",
        "            sid2 = np.hstack((sid2,bsid2))\n",
        "        sid1 = sid1.reshape(nd_dim1,nh_dim1,1)\n",
        "        sid2 = sid2.reshape(nd_dim2,nh_dim2,1)\n",
        "        nid1 = data[b'l_hood_indices']\n",
        "        nid2 = data[b'r_hood_indices']\n",
        "        edge_id1 = np.stack([sid1,nid1], axis=2)\n",
        "        edge_id1 = edge_id1.reshape(nd_dim1,nh_dim1,2)\n",
        "        edge_id2 = np.stack([sid2,nid2], axis=2)\n",
        "        edge_id2 = edge_id2.reshape(nd_dim2,nh_dim2,2)\n",
        "        edge_index1 = edge_id1.reshape(nd_dim1*nh_dim1,2)\n",
        "        edge_index1 = edge_index1.T\n",
        "        edge_index1 = torch.tensor(edge_index1, dtype=torch.long) # edge_index1\n",
        "        edge_index2 = edge_id2.reshape(nd_dim2*nh_dim2,2)\n",
        "        edge_index2 = edge_index2.T\n",
        "        edge_index2 = torch.tensor(edge_index2, dtype=torch.long) # edge_index2\n",
        "        ef1 = data[b'l_edge'].shape[2]\n",
        "        ef2 = data[b'r_edge'].shape[2]\n",
        "        edge_attr1 = data[b'l_edge'].reshape(nd_dim1*nh_dim1,ef1)\n",
        "        edge_attr2 = data[b'r_edge'].reshape(nd_dim2*nh_dim2,ef2)\n",
        "        edge_attr1 = torch.tensor(edge_attr1, dtype=torch.float) # edge_attr1\n",
        "        edge_attr2 = torch.tensor(edge_attr2, dtype=torch.float) # edge_attr2\n",
        "        pair_label = data[b'label'] #residual pair connectivity label\n",
        "        y = torch.tensor(pair_label, dtype=torch.float) # y\n",
        "\n",
        "        # datalist.append(Data(x1=x1,x2=x2,y=y, edge_index1=edge_index1, edge_index2=edge_index2, edge_attr1=edge_attr1, edge_attr2=edge_attr2))\n",
        "        datalist1.append(Data(x=x1,y=y, edge_index=edge_index1))\n",
        "        datalist2.append(Data(x=x2,y=y, edge_index=edge_index2))\n",
        "        \n",
        "    return datalist1, datalist2"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xqMABvKSHmre"
      },
      "source": [
        "train_datalist1, train_datalist2 = transformDataset(train_data)\n",
        "test_datalist1, test_datalist2 = transformDataset(test_data)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9m7sE9FHuvN"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wR8Ahkt9Hrdf"
      },
      "source": [
        "class PairGCN(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(PairGCN, self).__init__()\n",
        "        self.convs = nn.ModuleList()\n",
        "        self.convs.append(pyg_nn.GCNConv(input_dim, hidden_dim))\n",
        "        for l in range(2): # Can adjust\n",
        "            self.convs.append(pyg_nn.GCNConv(hidden_dim, hidden_dim))\n",
        "        # Linear layer: [node1_dim*node2_dim, 70+70] --> [node1_dim*node2_dim, 2]; hidden_dim * 2 = 70 + 70\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(hidden_dim * 2, hidden_dim * 2), nn.Dropout(0.5), \n",
        "            nn.Linear(hidden_dim * 2, output_dim))\n",
        "        self.num_layers = 2 # Can adjust\n",
        "        self.dropout = 0.5\n",
        "\n",
        "    def forward(self, data1, data2):\n",
        "        data1 = data1.to(device)\n",
        "        data2 = data2.to(device)\n",
        "        x1, edge_index1, y = data1.x, data1.edge_index, data1.y\n",
        "        x2, edge_index2 = data2.x, data2.edge_index\n",
        "\n",
        "        # y is only used to extract the example pairs\n",
        "\n",
        "        for i in range(self.num_layers):\n",
        "            x1 = self.convs[i](x1, edge_index1)\n",
        "            x2 = self.convs[i](x2, edge_index2) # The weights are shared\n",
        "            emb1 = x1\n",
        "            emb2 = x2\n",
        "            x1 = F.relu(x1)\n",
        "            x2 = F.relu(x2)\n",
        "            x1 = F.dropout(x1, p=self.dropout, training=self.training)\n",
        "            x2 = F.dropout(x2, p=self.dropout, training=self.training)\n",
        "\n",
        "        # Merge Residues Example Pairs\n",
        "        pairs = self.merge(x1, x2, y)\n",
        "\n",
        "        # Linear Layer Classification\n",
        "        out = self.classifier(pairs)\n",
        "\n",
        "        return [emb1,emb2], F.log_softmax(out, dim=1)\n",
        "\n",
        "    def merge(self, x1, x2, y):\n",
        "        pair_list = y[:,:-1]\n",
        "        left = pair_list[:,0].type(torch.long)\n",
        "        right = pair_list[:,1].type(torch.long)\n",
        "        return torch.hstack((x1[left],x2[right]))\n",
        "\n",
        "    def loss(self, pred, label):\n",
        "        label = label[:,-1]\n",
        "        label = label.type(torch.long)\n",
        "        label[label==-1]=0\n",
        "        weight = torch.tensor([1,10], dtype=torch.float).to(device) # To compensate for the uneven distribution of labels\n",
        "        return F.nll_loss(pred, label, weight)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SMrIUwLIH26K"
      },
      "source": [
        "def train(train_datalist1,train_datalist2, test_datalist1,test_datalist2, writer): # test_datalist added to check test scores\n",
        "    train_loader1 = DataLoader(train_datalist1, batch_size=128, shuffle=False)\n",
        "    train_loader2 = DataLoader(train_datalist2, batch_size=128, shuffle=False)\n",
        "    test_loader1 = DataLoader(test_datalist1, batch_size=128, shuffle=False)\n",
        "    test_loader2 = DataLoader(test_datalist2, batch_size=128, shuffle=False)\n",
        "\n",
        "    # build model\n",
        "    model = PairGCN(input_dim=70, hidden_dim=70, output_dim=2)\n",
        "    model = model.to(device)\n",
        "    opt = optim.Adam(model.parameters(), lr=0.01)\n",
        "    \n",
        "    # train\n",
        "    for epoch in range(1000):\n",
        "        total_loss = 0\n",
        "        model.train()\n",
        "        for (batch1, batch2) in zip(train_loader1, train_loader2):\n",
        "            opt.zero_grad()\n",
        "            embedding, pred = model(batch1, batch2)\n",
        "            label = batch1.y # labels are the same for both batches\n",
        "            loss = model.loss(pred, label)\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "            total_loss += loss.item() * batch1.num_graphs\n",
        "        total_loss /= len(train_loader1.dataset)\n",
        "        writer.add_scalar(\"loss\", total_loss, epoch)\n",
        "\n",
        "        if epoch % 50 == 0:\n",
        "            # test_acc = test(test_loader1, test_loader2, model)\n",
        "            test_acc = test(test_loader1, test_loader2, model)\n",
        "            print(\"Epoch {}. Loss: {:.4f}. Test AUC: {:.4f}\".format(\n",
        "                epoch, total_loss, test_acc))\n",
        "            writer.add_scalar(\"test AUC\", test_acc, epoch)\n",
        "\n",
        "    return model\n",
        "\n",
        "def test(loader1, loader2, model, is_validation=False):\n",
        "    model.eval()\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for (data1, data2) in zip(loader1, loader2):\n",
        "        data1 = data1.to(device)\n",
        "        data2 = data2.to(device)\n",
        "        # indices = torch.randperm(len(pictures))[:10]\n",
        "        with torch.no_grad():\n",
        "            emb, pred = model(data1, data2)\n",
        "            pred = pred.argmax(dim=1)\n",
        "            label = data1.y\n",
        "        label = label[:,-1]\n",
        "        label = label.type(torch.long)\n",
        "        label[label==-1]=0\n",
        "        # print(pred.sum())\n",
        "        # print(label.sum())\n",
        "        # correct += pred.eq(label).sum()#.item()\n",
        "        # total += len(label)\n",
        "        label = label.detach().cpu().numpy()\n",
        "        pred = pred.detach().cpu().numpy()\n",
        "    \n",
        "    # total = len(loader1.dataset)\n",
        "    return roc_auc_score(label, pred)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GdNYC-5vIBI8"
      },
      "source": [
        "# Train & Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z5VeWfkGH6J5",
        "outputId": "95922d93-2d85-46da-b350-956a3affed30"
      },
      "source": [
        "writer = SummaryWriter()\n",
        "\n",
        "model = train(train_datalist1, train_datalist2, test_datalist1, test_datalist2, writer)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0. Loss: 0.7014. Test AUC: 0.5000\n",
            "Epoch 50. Loss: 0.6852. Test AUC: 0.4750\n",
            "Epoch 100. Loss: 0.6804. Test AUC: 0.5004\n",
            "Epoch 150. Loss: 0.6792. Test AUC: 0.5095\n",
            "Epoch 200. Loss: 0.6779. Test AUC: 0.5015\n",
            "Epoch 250. Loss: 0.6772. Test AUC: 0.5417\n",
            "Epoch 300. Loss: 0.6801. Test AUC: 0.5006\n",
            "Epoch 350. Loss: 0.6738. Test AUC: 0.5119\n",
            "Epoch 400. Loss: 0.6714. Test AUC: 0.5355\n",
            "Epoch 450. Loss: 0.6729. Test AUC: 0.5264\n",
            "Epoch 500. Loss: 0.6753. Test AUC: 0.5356\n",
            "Epoch 550. Loss: 0.6694. Test AUC: 0.5375\n",
            "Epoch 600. Loss: 0.6712. Test AUC: 0.5439\n",
            "Epoch 650. Loss: 0.6724. Test AUC: 0.5234\n",
            "Epoch 700. Loss: 0.6701. Test AUC: 0.5216\n",
            "Epoch 750. Loss: 0.6675. Test AUC: 0.5173\n",
            "Epoch 800. Loss: 0.6723. Test AUC: 0.5191\n",
            "Epoch 850. Loss: 0.6673. Test AUC: 0.5150\n",
            "Epoch 900. Loss: 0.6733. Test AUC: 0.5078\n",
            "Epoch 950. Loss: 0.6679. Test AUC: 0.5266\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FwhQf83CId84"
      },
      "source": [
        "# Visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZdtaekymnYPt",
        "outputId": "2efe5116-5455-4951-f519-088fd8830b10"
      },
      "source": [
        "!tensorboard dev upload --logdir runs \\\n",
        "--name \"My latest experiment\" \\\n",
        "--description \"Simple comparison of several hyperparameters\""
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "***** TensorBoard Uploader *****\n",
            "\n",
            "This will upload your TensorBoard logs to https://tensorboard.dev/ from\n",
            "the following directory:\n",
            "\n",
            "runs\n",
            "\n",
            "This TensorBoard will be visible to everyone. Do not upload sensitive\n",
            "data.\n",
            "\n",
            "Your use of this service is subject to Google's Terms of Service\n",
            "<https://policies.google.com/terms> and Privacy Policy\n",
            "<https://policies.google.com/privacy>, and TensorBoard.dev's Terms of Service\n",
            "<https://tensorboard.dev/policy/terms/>.\n",
            "\n",
            "This notice will not be shown again while you are logged into the uploader.\n",
            "To log out, run `tensorboard dev auth revoke`.\n",
            "\n",
            "Continue? (yes/NO) yes\n",
            "\n",
            "Please visit this URL to authorize this application: https://accounts.google.com/o/oauth2/auth?response_type=code&client_id=373649185512-8v619h5kft38l4456nm2dj4ubeqsrvh6.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=openid+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fuserinfo.email&state=7f3oNB6EgAGLauFt73bZs7IB7oHXBw&prompt=consent&access_type=offline\n",
            "Enter the authorization code: 4/1AY0e-g5H91RYiVFz8dg9pMq-lmAcmadYUfr6nfkZjlBw6EukFgT8I7EyQEA\n",
            "\n",
            "Upload started and will continue reading any new data as it's added to the logdir.\n",
            "\n",
            "To stop uploading, press Ctrl-C.\n",
            "\n",
            "New experiment created. View your TensorBoard at: https://tensorboard.dev/experiment/yOvV3x7rS1qMAcwHVUVX4g/\n",
            "\n",
            "\u001b[1m[2020-12-20T11:47:11]\u001b[0m Started scanning logdir.\n",
            "\u001b[1m[2020-12-20T11:47:19]\u001b[0m Total uploaded: 7365 scalars, 0 tensors, 0 binary objects\n",
            "\u001b[2K\u001b[33mListening for new data in logdir...\u001b[0m\n",
            "\n",
            "Interrupted. View your TensorBoard at https://tensorboard.dev/experiment/yOvV3x7rS1qMAcwHVUVX4g/\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/tensorboard\", line 8, in <module>\n",
            "    sys.exit(run_main())\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorboard/main.py\", line 75, in run_main\n",
            "    app.run(tensorboard.main, flags_parser=tensorboard.configure)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/absl/app.py\", line 300, in run\n",
            "    _run_main(main, args)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/absl/app.py\", line 251, in _run_main\n",
            "    sys.exit(main(argv))\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorboard/program.py\", line 289, in main\n",
            "    return runner(self.flags) or 0\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorboard/uploader/uploader_subcommand.py\", line 671, in run\n",
            "    return _run(flags, self._experiment_url_callback)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorboard/uploader/uploader_subcommand.py\", line 140, in _run\n",
            "    intent.execute(server_info, channel)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/grpc/_channel.py\", line 1440, in __exit__\n",
            "    self._close()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/grpc/_channel.py\", line 1428, in _close\n",
            "    self._channel.close(cygrpc.StatusCode.cancelled, 'Channel closed!')\n",
            "  File \"src/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi\", line 515, in grpc._cython.cygrpc.Channel.close\n",
            "  File \"src/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi\", line 399, in grpc._cython.cygrpc._close\n",
            "  File \"src/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi\", line 429, in grpc._cython.cygrpc._close\n",
            "  File \"/usr/lib/python3.6/threading.py\", line 357, in notify_all\n",
            "    def notify_all(self):\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y-7AtkHE-T4T"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}